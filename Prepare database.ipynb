{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8221b0-8e6f-4870-8a90-b1638f64e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "from database_utils import (\n",
    "    split_texts_for_db,\n",
    "    calculate_embedding,\n",
    "    add_articles_to_qdrant,\n",
    "    add_article_to_qdrant,\n",
    "    assign_keywords,\n",
    "    classify_text,\n",
    ")\n",
    "from text_preparation_utils import (\n",
    "    sanitize_text,\n",
    "    drop_similar_rows,\n",
    ")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "QDRANT_KEY = os.getenv('QDRANT_KEY')\n",
    "QDRANT_CLUSTER_URL = os.getenv('QDRANT_CLUSTER_URL')\n",
    "HUGGING_FACE_TOKEN = os.getenv('HUGGING_FACE_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd525e4-e78b-4d79-aeef-82d1648bfef6",
   "metadata": {},
   "source": [
    "# Load base BBC dataset extended with additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9192e34-ed36-4f88-9b60-c789de94899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the preprocessed CSV file 'bbc_news_base.csv' into a DataFrame\n",
    "df_base = pd.read_csv('bbc_news_base.csv')\n",
    "\n",
    "# Drop unnecessary columns from the DataFrame\n",
    "df_base = df_base.drop(columns = ['Unnamed: 0', 'category_encoded', 'no_sentences', 'Flesch Reading Ease Score', 'Dale-Chall Readability Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "479fc19a-11ac-4858-ac1c-921ecec14d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>keywords</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit  Quarterly p...</td>\n",
       "      <td>business</td>\n",
       "      <td>['Time Warner', 'Quarterly profits', 'AOL', 'm...</td>\n",
       "      <td>Its profits were buoyed by one-off gains which...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech  The dollar h...</td>\n",
       "      <td>business</td>\n",
       "      <td>['Federal Reserve', 'Greenspan speech', 'highe...</td>\n",
       "      <td>The dollar has hit its highest level against t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim  The owners ...</td>\n",
       "      <td>business</td>\n",
       "      <td>['embattled Russian', 'Russian oil', 'unit buy...</td>\n",
       "      <td>The owners of embattled Russian oil giant Yuko...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits  British Air...</td>\n",
       "      <td>business</td>\n",
       "      <td>['British Airways', 'High fuel', 'blamed high'...</td>\n",
       "      <td>Looking ahead to its full year results to Marc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq  Shares in U...</td>\n",
       "      <td>business</td>\n",
       "      <td>['Allied Domecq', 'Domecq Shares', 'Pernod Ric...</td>\n",
       "      <td>Reports in the Wall Street Journal and the Fin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    labels  \\\n",
       "0  Ad sales boost Time Warner profit  Quarterly p...  business   \n",
       "1  Dollar gains on Greenspan speech  The dollar h...  business   \n",
       "2  Yukos unit buyer faces loan claim  The owners ...  business   \n",
       "3  High fuel prices hit BA's profits  British Air...  business   \n",
       "4  Pernod takeover talk lifts Domecq  Shares in U...  business   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  ['Time Warner', 'Quarterly profits', 'AOL', 'm...   \n",
       "1  ['Federal Reserve', 'Greenspan speech', 'highe...   \n",
       "2  ['embattled Russian', 'Russian oil', 'unit buy...   \n",
       "3  ['British Airways', 'High fuel', 'blamed high'...   \n",
       "4  ['Allied Domecq', 'Domecq Shares', 'Pernod Ric...   \n",
       "\n",
       "                                             summary  \n",
       "0  Its profits were buoyed by one-off gains which...  \n",
       "1  The dollar has hit its highest level against t...  \n",
       "2  The owners of embattled Russian oil giant Yuko...  \n",
       "3  Looking ahead to its full year results to Marc...  \n",
       "4  Reports in the Wall Street Journal and the Fin...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9df103f5-a3e8-4abe-b52b-e8e22fde3674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'entertainment', 'politics', 'sport', 'tech'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base.labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa460c4-d100-422e-a2b7-7e57e5e89d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function 'split_texts_for_db' to each element in the 'text' column\n",
    "df_base['splitted_text'] = df_base['text'].apply(lambda x: split_texts_for_db(x))\n",
    "\n",
    "# Apply the function 'calculate_embedding' to each element in the 'splitted_text' column\n",
    "df_base['embeddings'] = df_base['splitted_text'].apply(lambda x: calculate_embedding(x))\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify the changes\n",
    "df_base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaebc8a-1f8f-44a7-bd72-8e0626e5ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the URL of the BBC articles dataset to the 'data_source' column of the df_base DataFrame\n",
    "df_base['data_source'] = 'https://www.kaggle.com/datasets/jacopoferretti/bbc-articles-dataset'\n",
    "\n",
    "# Calling the function add_articles_to_qdrant to add articles from df_base to Qdrant using the provided key and cluster URL\n",
    "add_articles_to_qdrant(df_base, QDRANT_KEY, QDRANT_CLUSTER_URL, \"bbc_news_articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5466869a-e388-4d01-ba91-cddc61838b5b",
   "metadata": {},
   "source": [
    "# Read 2,225 articles published on the BBC News website during 2004-2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd81c1-79da-4492-99d6-af821a5b349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file into a DataFrame\n",
    "df_old = pd.read_csv('bbc-text.csv')\n",
    "\n",
    "# Sanitize the 'text' column by applying the 'sanitize_text' function to each entry\n",
    "df_old['text'] = df_old['text'].apply(lambda x: sanitize_text(x))\n",
    "\n",
    "# Remove rows with similar texts based on the 'text' column, allowing for 98% similarity\n",
    "print(f\"Before deletion of similar texts {len(df_old)}\")\n",
    "df_old = drop_similar_rows(df_old, 'text', 98)\n",
    "print(f\"After the deletion of similar texts {len(df_old)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c18f261-7e84-4b5e-a71a-6427d75737f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the assign_keywords function to each element in the 'text' column\n",
    "df_old['keywords'] = df_old['text'].apply(lambda x: assign_keywords(x))\n",
    "\n",
    "# Apply the split_texts_for_db function to each element in the 'text' column\n",
    "df_old['splitted_text'] = df_old['text'].apply(lambda x: split_texts_for_db(x))\n",
    "\n",
    "# Apply the calculate_embedding function to each element in the 'splitted_text' column\n",
    "df_old['embeddings'] = df_old['splitted_text'].apply(lambda x: calculate_embedding(x))\n",
    "\n",
    "df_old.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847a225-8bec-4878-9ae0-bf57a461c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a URL to the 'data_source' column of the df_old DataFrame\n",
    "df_old['data_source'] = 'http://mlg.ucd.ie/datasets/bbc.html'\n",
    "\n",
    "# Rename the 'category' column to 'labels' in the df_old DataFrame\n",
    "df_old.rename(columns = {'category': 'labels'}, inplace=True)\n",
    "\n",
    "# Call the function to add articles to Qdrant using the modified DataFrame and provided keys\n",
    "add_articles_to_qdrant(df_old, QDRANT_KEY, QDRANT_CLUSTER_UR, \"bbc_news_articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f2b909-a74b-4950-9cc9-fe1fbdafc41a",
   "metadata": {},
   "source": [
    "# Latest BBC News articles via dataset on Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2edbb0-bcd0-4ef6-8834-634b6b5bf16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log to huggingface for API connection\n",
    "login(HUGGING_FACE_TOKEN)\n",
    "\n",
    "# Load the dataset from a CSV file into a DataFrame\n",
    "df_hf = pd.read_parquet(\"hf://datasets/RealTimeData/bbc_latest/data/train-00000-of-00001.parquet\")\n",
    "df_hf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d061d6-4723-4ba5-bb6b-a0a82afa0a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List\n",
    "\n",
    "def process_articles(contents: List[str], titles: List[str], sources: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    Processes articles by sanitizing text, assigning keywords, splitting text,\n",
    "    calculating embeddings, classifying text, and adding the article to Qdrant.\n",
    "\n",
    "    Args:\n",
    "        contents (List[str]): List of article contents.\n",
    "        titles (List[str]): List of article titles.\n",
    "        sources (List[str]): List of article sources (links).\n",
    "    \"\"\"\n",
    "    for content, title, source in zip(contents, titles, sources):\n",
    "        text = title + content\n",
    "        sanitized_text = sanitize_text(text)\n",
    "        keywords = assign_keywords(text)\n",
    "        split_text = split_texts_for_db(text)\n",
    "        embeddings = calculate_embedding(split_text)\n",
    "        label = classify_text(sanitized_text)\n",
    "\n",
    "        row: Dict[str, Any] = {\n",
    "            \"text\": sanitized_text,\n",
    "            \"keywords\": keywords,\n",
    "            \"splitted_text\": split_text,\n",
    "            \"embeddings\": embeddings,\n",
    "            \"labels\": label,\n",
    "            \"data_source\": source\n",
    "        }\n",
    "\n",
    "        add_article_to_qdrant(row, QDRANT_KEY, QDRANT_CLUSTER_URL, \"bbc_news_articles\")\n",
    "\n",
    "\n",
    "# Extract the 'content' column from the DataFrame and convert it to a list\n",
    "contents = df_hf['content'].tolist()\n",
    "\n",
    "# Extract the 'title' column from the DataFrame and convert it to a list\n",
    "titles = df_hf['title'].tolist()\n",
    "\n",
    "# Extract the 'link' column from the DataFrame and convert it to a list\n",
    "sources = df_hf['link'].tolist()\n",
    "\n",
    "# Process the articles using the extracted contents, titles, and sources\n",
    "process_articles(contents, titles, sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d27e30-e9c3-4233-9a36-2d5a07421691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "ds = datasets.load_dataset('RealTimeData/bbc_latest', revision = '2023-08-20')\n",
    "df_hf = ds['train'].to_pandas()\n",
    "\n",
    "# Extract the 'content' column from the DataFrame and convert it to a list\n",
    "contents = df_hf['content'].tolist()\n",
    "\n",
    "# Extract the 'title' column from the DataFrame and convert it to a list\n",
    "titles = df_hf['title'].tolist()\n",
    "\n",
    "# Extract the 'link' column from the DataFrame and convert it to a list\n",
    "sources = df_hf['link'].tolist()\n",
    "\n",
    "# Process the articles using the extracted contents, titles, and sources\n",
    "process_articles(contents, titles, sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8c4320-10d5-485e-b8eb-c062baf06ccd",
   "metadata": {},
   "source": [
    "# Dataset with only urls having 35,860 rows from 07 March 2022 to 03 July 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395de3d6-9fe5-43f0-82a2-67acabc5c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urls = pd.read_csv('bbc_news.csv')\n",
    "df_urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c1766-80e9-4079-a1aa-cb2b60b96f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ver_1.iloc[2].link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d79b5-5e83-4bb9-b1e3-1ba88084fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ver_1.iloc[2].description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca932e11-bcd3-401a-9e21-675b7fb89aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_bbc_article(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        # Parse the HTML content of the page\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extract the headline\n",
    "        headline = soup.find('h1').get_text(strip=True) if soup.find('h1') else 'No headline found'\n",
    "\n",
    "        # Extract the article body\n",
    "        article_body = soup.find('article')  # Target the main article container\n",
    "        paragraphs = article_body.find_all('p') if article_body else []\n",
    "        # List of prefixes to ignore\n",
    "        ignore_prefixes = [\"LIVE:\", \"IN KYIV:\", \"ANALYSIS:\", \"EXPLAINED:\", \"IN DEPTH:\"]\n",
    "\n",
    "        # Filter out paragraphs that start with any of the prefixes\n",
    "        filtered_paragraphs = []\n",
    "        for p in paragraphs:\n",
    "            text = p.get_text(strip=True)\n",
    "            if not any(text.startswith(prefix) for prefix in ignore_prefixes):\n",
    "                filtered_paragraphs.append(text)\n",
    "\n",
    "        # Combine the filtered paragraphs\n",
    "        article_text = \"\\n\".join(filtered_paragraphs)\n",
    "\n",
    "        # Return the headline and article text\n",
    "        return headline, article_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa65fa-ed96-40f4-b0b6-afb83693160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get texts of articles\n",
    "#for i, row in df_ver_1.iterrows():\n",
    "#    url = row['link']\n",
    "#    try:\n",
    "#        headline, article_text = scrape_bbc_article(url)\n",
    "#    except Exception as e:\n",
    "#        print(e)\n",
    "#    df_ver_1.loc[i, \"text\"]= article_text\n",
    "#    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025db9c6-7251-4426-af91-733d8213874a",
   "metadata": {},
   "source": [
    "# Data Sources\n",
    "\n",
    "1. https://www.kaggle.com/datasets/jacopoferretti/bbc-articles-dataset\n",
    "2. https://www.kaggle.com/datasets/bhavikjikadara/bbc-news-articles\n",
    "3. https://huggingface.co/datasets/SetFit/bbc-news\n",
    "4. http://mlg.ucd.ie/datasets/bbc.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:text-mining] *",
   "language": "python",
   "name": "conda-env-text-mining-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
